{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:42:22.160766Z",
     "start_time": "2026-02-01T06:42:20.798166Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chris\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: keras in c:\\users\\chris\\anaconda3\\lib\\site-packages (3.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (6.33.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: rich in c:\\users\\chris\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\chris\\anaconda3\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\chris\\anaconda3\\lib\\site-packages (from keras) (0.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib tensorflow keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede39164e4d3808",
   "metadata": {},
   "source": [
    "Base Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d837508538eb300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:42:22.163961Z",
     "start_time": "2026-02-01T06:42:22.161352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary mapping names to lambda functions\n",
    "# Signature: func(n_inputs, n_neurons) -> np.array\n",
    "INIT_STRATEGIES = {\n",
    "    \"he\": lambda i, o: np.random.randn(i, o) * np.sqrt(2. / i),\n",
    "    \"uniform\": lambda i, o: np.random.uniform(-1 / np.sqrt(i), 1 / np.sqrt(i), (i, o)),\n",
    "    \"normal\": lambda i, o: np.random.randn(i, o) * 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1b08156b00477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:52:59.833305Z",
     "start_time": "2026-02-01T06:52:59.833305Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# --- 1. Linear Layer---\n",
    "class LinearLayer:\n",
    "    def __init__(self, n_inputs, n_neurons, init_fn):\n",
    "        \"\"\"\n",
    "        init_fn: A function that accepts (n_inputs, n_neurons) \n",
    "                 and returns a weight matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.W = init_fn(n_inputs, n_neurons)\n",
    "        self.b = np.zeros((1, n_neurons))\n",
    "\n",
    "        # Velocity for Momentum\n",
    "        self.v_W = np.zeros_like(self.W)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "\n",
    "        # Cache\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "\n",
    "    def backward(self, d_out, alpha, gamma):\n",
    "        self.dW = np.dot(self.x.T, d_out)\n",
    "        self.db = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_out, self.W.T)\n",
    "\n",
    "        # Update with Momentum\n",
    "        self.v_W = (gamma * self.v_W) + (alpha * self.dW)\n",
    "        self.v_b = (gamma * self.v_b) + (alpha * self.db)\n",
    "        self.W -= self.v_W\n",
    "        self.b -= self.v_b\n",
    "\n",
    "        return d_input\n",
    "\n",
    "\n",
    "# --- 2. ReLU Activation ---\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, d_out,alpha,gamma):\n",
    "        # Pass gradient through only if x was > 0\n",
    "        d_input = d_out.copy()\n",
    "        d_input[self.x <= 0] = 0\n",
    "        return d_input\n",
    "\n",
    "\n",
    "# --- 3. The Neural Network Manager ---\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, init_strategy=\"he\"):\n",
    "\n",
    "        # 1. Select the specific lambda function\n",
    "        if init_strategy not in INIT_STRATEGIES:\n",
    "            raise ValueError(f\"Strategy must be one of {list(INIT_STRATEGIES.keys())}\")\n",
    "\n",
    "        weight_fn = INIT_STRATEGIES[init_strategy]\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        # Helper to make adding layers cleaner\n",
    "        def add_block(n_in, n_out):\n",
    "            # Pass the lambda (weight_fn) to the layer\n",
    "            self.layers.append(LinearLayer(n_in, n_out, weight_fn))\n",
    "            self.layers.append(ReLU())\n",
    "\n",
    "        # --- Build Architecture ---\n",
    "\n",
    "        # Input to Hidden 1\n",
    "        add_block(input_size, hidden_size)\n",
    "\n",
    "        # Hidden to Hidden (4 blocks)\n",
    "        for _ in range(1):\n",
    "            add_block(hidden_size, hidden_size)\n",
    "\n",
    "        self.layers.append(LinearLayer(hidden_size, output_size, weight_fn))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, LinearLayer) or isinstance(layer, ReLU):\n",
    "                out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def softmax(self, z):\n",
    "        # Subtract max for numerical stability (prevents exploding exponentials)\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def train_step(self, X_batch, y_batch, alpha, gamma):\n",
    "        # 1. Forward Pass\n",
    "        logits = self.forward(X_batch)\n",
    "        \n",
    "        #predict\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        # 2. Backward Pass\n",
    "        batch_size = X_batch.shape[0]\n",
    "        \n",
    "        # average error based on batch_size\n",
    "        d_out = (probs - y_batch) / batch_size\n",
    "\n",
    "        # Propagate error backwards\n",
    "        for layer in reversed(self.layers):\n",
    "            #alpha = learning rate gamma = momentum rate\n",
    "            d_out = layer.backward(d_out, alpha, gamma)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        # Helper to check accuracy\n",
    "        logits = self.forward(X)\n",
    "        probs = self.softmax(logits)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        truth = np.argmax(y, axis=1)\n",
    "        return np.mean(preds == truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e01409724e23a5",
   "metadata": {},
   "source": [
    "Running training on the Z normalized dataset (fully supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567362f554f19420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:54:33.533221Z",
     "start_time": "2026-02-01T06:53:01.953463Z"
    }
   },
   "outputs": [],
   "source": "from DataLoader import DataLoader\nfrom Overfitting import OverfitDetector\nfrom Results import Results\nimport numpy as np\nimport pandas as pd\n\n# ---------------------------------------------------------\n# A. Load and Split Data (supervised, 80% of dataset)\n# ---------------------------------------------------------\nloader = DataLoader()\n\nprint(\"\\n1. Getting Z-Score Normalized Data...\")\ndf_zscore = loader.get_z_score_df()\n\nprint(\"\\n2. Splitting\")\ndata_dict = loader.get_supervised_split(df_zscore, dataset_pct=0.8)\n\noriginal_train = data_dict[\"train\"]\nprint(f\"   Original Labeled Size: {original_train.shape[0]}\")\n\n# ---------------------------------------------------------\n# B. Augmentation\n# ---------------------------------------------------------\nprint(\"\\n3. Augmenting Labeled Data (Horizontal Flip)...\")\nflipped_train = loader.augment_horizontal_flip(original_train)\nfinal_train_df = pd.concat([original_train, flipped_train], axis=0).reset_index(drop=True)\nprint(f\"   Final Training Size (Original + Flipped): {final_train_df.shape[0]}\")\n\n# ---------------------------------------------------------\n# C. Convert DataFrames to NumPy Arrays\n# ---------------------------------------------------------\nX_train, y_train = loader.to_numpy(final_train_df)\nX_val, y_val     = loader.to_numpy(data_dict[\"validation\"])\nX_test, y_test   = loader.to_numpy(data_dict[\"test\"])\n\n# ---------------------------------------------------------\n# D. Training Loop (manual numpy NeuralNetwork from above)\n# ---------------------------------------------------------\nprint(\"\\n4. Initializing Network...\")\nnn = NeuralNetwork(input_size=784, hidden_size=1028, output_size=10, init_strategy=\"he\")\n\nBATCH_SIZE = 64\nEPOCHS = 200\nALPHA = 0.01\nGAMMA = 0.04\n\nresults = Results(\"results.csv\")\nresults.begin_run(seed=42, weight_init=\"he\", normalization=\"z_score\",\n                  augmentations=\"horizontal_flip\")\n\noverfit_detector = OverfitDetector()\nconverged_epoch = EPOCHS\n\nprint(f\"   Starting Training (Batch={BATCH_SIZE}, Alpha={ALPHA}, Gamma={GAMMA})...\")\nnum_batches = int(X_train.shape[0] / BATCH_SIZE)\n\nfor epoch in range(EPOCHS):\n    perm = np.random.permutation(X_train.shape[0])\n    X_shuffled = X_train[perm]\n    y_shuffled = y_train[perm]\n\n    for b in range(num_batches):\n        start = b * BATCH_SIZE\n        end = start + BATCH_SIZE\n        nn.train_step(X_shuffled[start:end], y_shuffled[start:end], ALPHA, GAMMA)\n\n    # Validation\n    val_acc = nn.evaluate(X_val, y_val)\n    train_acc = nn.evaluate(X_train[:5000], y_train[:5000])  # sample for speed\n    val_error = 1.0 - val_acc\n\n    results.log_epoch(epoch + 1, train_loss=0.0, train_acc=train_acc,\n                      val_loss=0.0, val_acc=val_acc)\n\n    is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error)\n\n    if (epoch + 1) % 10 == 0:\n        status = \" ** OVERFIT **\" if is_overfitting else \"\"\n        print(f\"   Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}{status}\")\n\n    if is_overfitting:\n        converged_epoch = epoch + 1\n        print(f\"   >>> Overfitting at epoch {converged_epoch}.\")\n        break\n\n# ---------------------------------------------------------\n# E. Final Test\n# ---------------------------------------------------------\ntest_acc = nn.evaluate(X_test, y_test)\nresults.end_run(test_acc=test_acc, converged_epoch=converged_epoch)\n\nprint(f\"\\nFinal Test Accuracy (Numpy Vanilla, Z-Score, He init): {test_acc:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "b2ba16a4ffdf73f7",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24e498b5a22bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:43:11.200974Z",
     "start_time": "2026-02-01T06:43:11.200974Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define label names for Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Pick 5 random indices from the test set\n",
    "# We use len(X_test) from the previous step where we prepared the Z-score data\n",
    "indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "print(\"--- Visualizing Predictions on Z-Score Normalized Data ---\")\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img = X_test[idx]  # Shape: (784,)\n",
    "\n",
    "    # 1. Forward pass to get prediction\n",
    "    # We reshape to (1, 784) because the network expects a batch of inputs\n",
    "    logits = nn.forward(img.reshape(1, -1))\n",
    "\n",
    "    # 2. Softmax to get probabilities\n",
    "    probs = nn.softmax(logits)\n",
    "    prediction = np.argmax(probs)\n",
    "\n",
    "    # 3. Get True Label\n",
    "    # y_test is one-hot encoded, so we use argmax to get the index (0-9)\n",
    "    true_label = np.argmax(y_test[idx])\n",
    "\n",
    "    # 4. Plot\n",
    "    # Reshape back to 28x28 for the image\n",
    "    axes[i].imshow(img.reshape(28, 28), cmap='gray')\n",
    "\n",
    "    # Set title color: Green if correct, Red if wrong\n",
    "    title_color = 'green' if prediction == true_label else 'red'\n",
    "\n",
    "    axes[i].set_title(f\"Pred: {class_names[prediction]}\\nTrue: {class_names[true_label]}\", color=title_color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a263e02db68e941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:43:11.202153Z",
     "start_time": "2026-02-01T06:43:11.202153Z"
    }
   },
   "outputs": [],
   "source": [
    "#Still must integrate momentum into the NN, and do the other weight initializations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de6f25a422624a",
   "metadata": {},
   "source": [
    "Next, I will integrate the validation metrics, like overfitting threshold, and do the stopiing condition, then maybe put it for some hyperparam tuning like weight decay or learning rate decay eventually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v6gtt7wvgam",
   "metadata": {},
   "source": [
    "## Overfitting Detection\n",
    "\n",
    "**Criterion:** $E_V^{(t)} > \\bar{E}_V + \\sigma_{E_V}$\n",
    "\n",
    "At epoch $t$, if the current validation error exceeds the running mean plus one standard deviation, the model is considered to be overfitting.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $E_V^{(t)}$ | Validation error at epoch $t$ |\n",
    "| $\\bar{E}_V = \\frac{1}{t} \\sum_{i=1}^{t} E_V^{(i)}$ | Running mean of all validation errors so far |\n",
    "| $\\sigma_{E_V} = \\sqrt{\\frac{1}{t} \\sum_{i=1}^{t} (E_V^{(i)} - \\bar{E}_V)^2}$ | Running standard deviation |\n",
    "\n",
    "**Intuition:** The validation error naturally fluctuates. If it spikes above the historical mean by more than one standard deviation, the model has likely begun memorizing the training data rather than learning generalizable patterns.\n",
    "\n",
    "\n",
    "**Mods** Include a scalar multiple of the runnning std to terminate off, perhaps 2*$\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bu6jyceu285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | E_V   | E_V_bar | sigma  | Threshold | Overfit?\n",
      "------+-------+---------+--------+-----------+---------\n",
      "  01  | 0.300 |  0.300  | 0.000 |   0.300   |  False\n",
      "  02  | 0.250 |  0.275  | 0.025 |   0.300   |  False\n",
      "  03  | 0.200 |  0.250  | 0.041 |   0.291   |  False\n",
      "  04  | 0.180 |  0.232  | 0.047 |   0.279   |  False\n",
      "  05  | 0.160 |  0.218  | 0.051 |   0.269   |  False\n",
      "  06  | 0.150 |  0.207  | 0.053 |   0.259   |  False\n",
      "  07  | 0.140 |  0.197  | 0.054 |   0.251   |  False\n",
      "  08  | 0.130 |  0.189  | 0.055 |   0.244   |  False\n",
      "  09  | 0.130 |  0.182  | 0.055 |   0.238   |  False\n",
      "  10  | 0.300 |  0.194  | 0.063 |   0.257   |  True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OverfitDetector:\n",
    "    \"\"\"\n",
    "    Detects overfitting using:  E_V(t) > mean(E_V) + std(E_V)\n",
    "    \n",
    "    Tracks all validation errors and checks if the current one\n",
    "    exceeds the running mean by more than one standard deviation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.val_errors = []\n",
    "    \n",
    "    def check(self, val_error):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            val_error: validation error (1 - val_accuracy) at current epoch\n",
    "        Returns:\n",
    "            is_overfitting (bool), mean_ev, std_ev\n",
    "        \"\"\"\n",
    "        self.val_errors.append(val_error)\n",
    "        \n",
    "        # Need at least 2 points to compute a meaningful std\n",
    "        if len(self.val_errors) < 2:\n",
    "            return False, val_error, 0.0\n",
    "        \n",
    "        # E_V_bar = (1/t) * sum(E_V)\n",
    "        mean_ev = np.mean(self.val_errors)\n",
    "        \n",
    "        # sigma_EV = sqrt( (1/t) * sum( (E_V - E_V_bar)^2 ) )\n",
    "        std_ev = np.std(self.val_errors)\n",
    "        \n",
    "        threshold = mean_ev + std_ev\n",
    "        is_overfitting = val_error > threshold\n",
    "        \n",
    "        return is_overfitting, mean_ev, std_ev\n",
    "\n",
    "# --- Quick sanity test ---\n",
    "detector = OverfitDetector()\n",
    "\n",
    "# Simulate: error decreases then spikes\n",
    "fake_errors = [0.30, 0.25, 0.20, 0.18, 0.16, 0.15, 0.14, 0.13, 0.13, 0.30]\n",
    "\n",
    "print(\"Epoch | E_V   | E_V_bar | sigma  | Threshold | Overfit?\")\n",
    "print(\"------+-------+---------+--------+-----------+---------\")\n",
    "for i, ev in enumerate(fake_errors):\n",
    "    overfit, mean_ev, std_ev = detector.check(ev)\n",
    "    threshold = mean_ev + std_ev\n",
    "    print(f\"  {i+1:02d}  | {ev:.3f} |  {mean_ev:.3f}  | {std_ev:.3f} |   {threshold:.3f}   |  {overfit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9f12e27253b75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T06:56:52.257547Z",
     "start_time": "2026-02-01T06:56:22.228672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. Shape: (70000, 785)\n",
      "\n",
      "1. Getting Z-Score Normalized Data...\n",
      "\n",
      "2. Splitting...\n",
      "   Original Labeled Size: 44800\n",
      "\n",
      "3. Augmenting Labeled Data (Horizontal Flip)...\n",
      "   Final Training Size (Original + Flipped): 89600\n",
      "\n",
      "4. Initializing Network...\n",
      "   Using device: cuda\n",
      "   Starting Training (Batch=64, Alpha=0.01, Gamma=0.04)...\n",
      "   Epoch 010 | Val Acc: 0.8920 | E_V: 0.1080 | E_V_bar: 0.1080 | sigma: 0.0000 | Thresh: 0.1080 \n",
      "   Epoch 020 | Val Acc: 0.8945 | E_V: 0.1055 | E_V_bar: 0.1068 | sigma: 0.0012 | Thresh: 0.1080 \n",
      "   Epoch 030 | Val Acc: 0.8936 | E_V: 0.1064 | E_V_bar: 0.1067 | sigma: 0.0010 | Thresh: 0.1077 \n",
      "   Epoch 040 | Val Acc: 0.8962 | E_V: 0.1038 | E_V_bar: 0.1059 | sigma: 0.0015 | Thresh: 0.1075 \n",
      "   Epoch 050 | Val Acc: 0.8950 | E_V: 0.1050 | E_V_bar: 0.1057 | sigma: 0.0014 | Thresh: 0.1072 \n",
      "   Epoch 060 | Val Acc: 0.8955 | E_V: 0.1045 | E_V_bar: 0.1055 | sigma: 0.0014 | Thresh: 0.1069 \n",
      "   Epoch 070 | Val Acc: 0.8954 | E_V: 0.1046 | E_V_bar: 0.1054 | sigma: 0.0013 | Thresh: 0.1067 \n",
      "   Epoch 080 | Val Acc: 0.8959 | E_V: 0.1041 | E_V_bar: 0.1052 | sigma: 0.0013 | Thresh: 0.1066 \n",
      "   Epoch 090 | Val Acc: 0.8966 | E_V: 0.1034 | E_V_bar: 0.1050 | sigma: 0.0014 | Thresh: 0.1064 \n",
      "   Epoch 100 | Val Acc: 0.8966 | E_V: 0.1034 | E_V_bar: 0.1049 | sigma: 0.0014 | Thresh: 0.1063 \n",
      "   Epoch 110 | Val Acc: 0.8970 | E_V: 0.1030 | E_V_bar: 0.1047 | sigma: 0.0014 | Thresh: 0.1061 \n",
      "   Epoch 120 | Val Acc: 0.8964 | E_V: 0.1036 | E_V_bar: 0.1046 | sigma: 0.0014 | Thresh: 0.1060 \n",
      "   Epoch 130 | Val Acc: 0.8955 | E_V: 0.1045 | E_V_bar: 0.1046 | sigma: 0.0013 | Thresh: 0.1059 \n",
      "   Epoch 140 | Val Acc: 0.8962 | E_V: 0.1038 | E_V_bar: 0.1045 | sigma: 0.0013 | Thresh: 0.1059 \n",
      "   Epoch 150 | Val Acc: 0.8968 | E_V: 0.1032 | E_V_bar: 0.1045 | sigma: 0.0013 | Thresh: 0.1058 \n",
      "   Epoch 160 | Val Acc: 0.8966 | E_V: 0.1034 | E_V_bar: 0.1044 | sigma: 0.0013 | Thresh: 0.1057 \n",
      "   Epoch 170 | Val Acc: 0.8962 | E_V: 0.1038 | E_V_bar: 0.1043 | sigma: 0.0013 | Thresh: 0.1056 \n",
      "   Epoch 180 | Val Acc: 0.8959 | E_V: 0.1041 | E_V_bar: 0.1043 | sigma: 0.0012 | Thresh: 0.1056 \n",
      "   Epoch 190 | Val Acc: 0.8961 | E_V: 0.1039 | E_V_bar: 0.1043 | sigma: 0.0012 | Thresh: 0.1055 \n",
      "   Epoch 200 | Val Acc: 0.8962 | E_V: 0.1038 | E_V_bar: 0.1043 | sigma: 0.0012 | Thresh: 0.1055 \n",
      "\n",
      "Final Test Accuracy: 0.8873\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader as TorchDataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from DataLoader import DataLoader\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Weight Initialization Strategies \n",
    "# ---------------------------------------------------------\n",
    "TORCH_INIT_STRATEGIES = {\n",
    "    \"he\": lambda w: nn.init.kaiming_normal_(w, mode='fan_in', nonlinearity='relu'),\n",
    "    \"uniform\": lambda w: nn.init.uniform_(w, -1 / np.sqrt(w.shape[1]), 1 / np.sqrt(w.shape[1])),\n",
    "    \"normal\": lambda w: nn.init.normal_(w, mean=0.0, std=0.01),\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# A. Custom Dataset Class\n",
    "# ---------------------------------------------------------\n",
    "class dataset_to_torch(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        # Convert one-hot encoded labels to class indices for CrossEntropyLoss\n",
    "        self.y = torch.LongTensor(np.argmax(y, axis=1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# B. Neural Network Definition (matches manual numpy version)\n",
    "#    Architecture: Linear -> ReLU -> Linear -> ReLU -> Linear\n",
    "# ---------------------------------------------------------\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=1028, output_size=10, init_strategy=\"he\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        if init_strategy not in TORCH_INIT_STRATEGIES:\n",
    "            raise ValueError(f\"Strategy must be one of {list(TORCH_INIT_STRATEGIES.keys())}\")\n",
    "        \n",
    "        # 2 hidden layers + output, matching the manual version\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "        # Apply selected weight initialization\n",
    "        weight_fn = TORCH_INIT_STRATEGIES[init_strategy]\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                weight_fn(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# C. Load and Split Data\n",
    "# ---------------------------------------------------------\n",
    "loader = DataLoader()\n",
    "\n",
    "print(\"\\n1. Getting Z-Score Normalized Data...\")\n",
    "df_zscore = loader.get_z_score_df()\n",
    "\n",
    "print(\"\\n2. Splitting...\")\n",
    "data_dict = loader.get_supervised_split(df_zscore, dataset_pct=0.8)\n",
    "\n",
    "original_train = data_dict[\"train\"]\n",
    "print(f\"   Original Labeled Size: {original_train.shape[0]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# D. Augmentation\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n3. Augmenting Labeled Data (Horizontal Flip)...\")\n",
    "flipped_train = loader.augment_horizontal_flip(original_train)\n",
    "\n",
    "final_train_df = pd.concat([original_train, flipped_train], axis=0).reset_index(drop=True)\n",
    "print(f\"   Final Training Size (Original + Flipped): {final_train_df.shape[0]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# E. Convert to NumPy and Create PyTorch Datasets\n",
    "# ---------------------------------------------------------\n",
    "X_train, y_train = loader.to_numpy(final_train_df)\n",
    "X_val, y_val = loader.to_numpy(data_dict[\"validation\"])\n",
    "X_test, y_test = loader.to_numpy(data_dict[\"test\"])\n",
    "\n",
    "train_dataset = dataset_to_torch(X_train, y_train)\n",
    "val_dataset = dataset_to_torch(X_val, y_val)\n",
    "test_dataset = dataset_to_torch(X_test, y_test)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# F. Create Data Loaders\n",
    "# ---------------------------------------------------------\n",
    "BATCH_SIZE = 64\n",
    "import platform\n",
    "NUM_WORKERS = 0 if platform.system() == 'Windows' else 2\n",
    "\n",
    "train_loader = TorchDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader = TorchDataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "test_loader = TorchDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# G. Training Setup (matching manual hyperparams exactly)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Initializing Network...\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"   Using device: {device}\")\n",
    "\n",
    "model = NeuralNetwork(input_size=784, hidden_size=1028, output_size=10, init_strategy=\"he\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 200\n",
    "ALPHA = 0.01   # Learning Rate\n",
    "GAMMA = 0.04   # Momentum\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=ALPHA, momentum=GAMMA)\n",
    "\n",
    "# Overfitting detector: E_V > E_V_bar + sigma_EV\n",
    "overfit_detector = OverfitDetector()\n",
    "\n",
    "print(f\"   Starting Training (Batch={BATCH_SIZE}, Alpha={ALPHA}, Gamma={GAMMA})...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# H. Training Loop\n",
    "# ---------------------------------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        val_error = 1.0 - val_acc\n",
    "        \n",
    "        # Check overfitting: E_V > E_V_bar + sigma_EV\n",
    "        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error)\n",
    "        threshold = mean_ev + std_ev\n",
    "        \n",
    "        status = \"** OVERFITTING **\" if is_overfitting else \"\"\n",
    "        print(f\"   Epoch {epoch + 1:03d} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"E_V: {val_error:.4f} | E_V_bar: {mean_ev:.4f} | \"\n",
    "              f\"sigma: {std_ev:.4f} | Thresh: {threshold:.4f} {status}\")\n",
    "        \n",
    "        if is_overfitting:\n",
    "            print(f\"   Stopping: E_V({val_error:.4f}) > E_V_bar + sigma({threshold:.4f})\")\n",
    "            break\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# I. Final Test\n",
    "# ---------------------------------------------------------\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += batch_y.size(0)\n",
    "        test_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nhcihgle7vi",
   "metadata": {},
   "source": [
    "## Stage 2e: Baseline Training (10% Labeled Only)\n",
    "\n",
    "Per the assignment:\n",
    "- **Data split:** 10% test, 10% validation, 80% training pool (from all 70k)\n",
    "- **Semi-supervised setup:** From training pool, 20% labeled / 80% unlabeled\n",
    "- **Baseline:** Train using **only** the labeled portion (~11,200 samples)\n",
    "- **Seeds:** {1, 123, 12345} — run 3 times, record mean ± std\n",
    "- **Record:** Training/validation loss & accuracy curves, final test accuracy, epochs to convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amyp06kqb3",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom DataLoader import DataLoader\nfrom VanillaModel import VanillaModel\nfrom Overfitting import OverfitDetector\nfrom Results import Results\n\n# ---------------------------------------------------------\n# Seed-setting utility\n# ---------------------------------------------------------\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# ---------------------------------------------------------\n# Results pipeline\n# ---------------------------------------------------------\nresults = Results(\"results.csv\")\n\n# ---------------------------------------------------------\n# Single training run\n# ---------------------------------------------------------\ndef run_baseline(seed, init_strategy=\"he\"):\n    set_seed(seed)\n\n    results.begin_run(\n        seed=seed,\n        weight_init=init_strategy,\n        normalization=\"z_score\",\n        augmentations=\"horizontal_flip\"\n    )\n\n    # A. Load, normalize, split\n    loader = DataLoader()\n    df_zscore = loader.get_z_score_df()\n    data_dict = loader.get_semi_supervised_split(\n        df_zscore, test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=seed\n    )\n\n    labeled_train = data_dict[\"labeled_train\"]\n    print(f\"   Seed {seed} | Labeled train: {len(labeled_train)} | \"\n          f\"Val: {len(data_dict['validation'])} | Test: {len(data_dict['test'])}\")\n\n    # B. Augmentation\n    flipped = loader.augment_horizontal_flip(labeled_train)\n    final_train = pd.concat([labeled_train, flipped], axis=0).reset_index(drop=True)\n\n    # C. Build torch loaders via DataLoader helper\n    loaders = loader.to_torch_loaders({\n        \"train\": final_train,\n        \"validation\": data_dict[\"validation\"],\n        \"test\": data_dict[\"test\"]\n    }, batch_size=64)\n    train_loader = loaders[\"train\"]\n    val_loader = loaders[\"validation\"]\n    test_loader = loaders[\"test\"]\n\n    # D. Model setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    set_seed(seed)\n    model = VanillaModel(init_strategy=init_strategy).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    ALPHA = 0.01\n    GAMMA = 0.04\n    optimizer = optim.SGD(model.parameters(), lr=ALPHA, momentum=GAMMA)\n\n    # E. Training loop\n    EPOCHS = 200\n    overfit_detector = OverfitDetector()\n    converged_epoch = EPOCHS\n\n    for epoch in range(EPOCHS):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_total += batch_y.size(0)\n            train_correct += (predicted == batch_y).sum().item()\n\n        epoch_train_loss = train_loss / len(train_loader)\n        epoch_train_acc = train_correct / train_total\n\n        # --- Validate ---\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_total += batch_y.size(0)\n                val_correct += (predicted == batch_y).sum().item()\n\n        epoch_val_loss = val_loss / len(val_loader)\n        epoch_val_acc = val_correct / val_total\n\n        results.log_epoch(epoch + 1, epoch_train_loss, epoch_train_acc,\n                          epoch_val_loss, epoch_val_acc)\n\n        val_error = 1.0 - epoch_val_acc\n        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error)\n\n        if (epoch + 1) % 10 == 0:\n            status = \" ** OVERFIT **\" if is_overfitting else \"\"\n            print(f\"   Epoch {epoch+1:03d} | Train Loss: {epoch_train_loss:.4f} | \"\n                  f\"Train Acc: {epoch_train_acc:.4f} | Val Acc: {epoch_val_acc:.4f}{status}\")\n\n        if is_overfitting:\n            converged_epoch = epoch + 1\n            print(f\"   >>> Overfitting at epoch {converged_epoch}. \"\n                  f\"E_V={val_error:.4f} > threshold={mean_ev + std_ev:.4f}\")\n            break\n\n    # F. Test\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    with torch.no_grad():\n        for batch_X, batch_y in test_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            outputs = model(batch_X)\n            _, predicted = torch.max(outputs, 1)\n            test_total += batch_y.size(0)\n            test_correct += (predicted == batch_y).sum().item()\n\n    test_acc = test_correct / test_total\n    results.end_run(test_acc=test_acc, converged_epoch=converged_epoch)\n\n    return {\"seed\": seed, \"test_acc\": test_acc, \"converged_epoch\": converged_epoch}\n\n# ---------------------------------------------------------\n# Run 3 seeds\n# ---------------------------------------------------------\nSEEDS = [1, 123, 12345]\nall_results = []\n\nfor seed in SEEDS:\n    print(f\"{'='*60}\")\n    print(f\"Running baseline with seed={seed}\")\n    print(f\"{'='*60}\")\n    result = run_baseline(seed, init_strategy=\"he\")\n    all_results.append(result)\n\n# ---------------------------------------------------------\n# Summary\n# ---------------------------------------------------------\nprint(f\"\\n{'='*60}\")\nprint(f\"BASELINE RESULTS (He init, 10% labeled only)\")\nprint(f\"{'='*60}\")\nresults.summary()\n\n# ---------------------------------------------------------\n# Plot from CSV\n# ---------------------------------------------------------\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfor i, r in enumerate(all_results):\n    run_df = results.get_run(r[\"seed\"], \"he\")\n\n    axes[0][i].plot(run_df[\"epoch\"], run_df[\"train_loss\"], label=\"Train Loss\")\n    axes[0][i].plot(run_df[\"epoch\"], run_df[\"val_loss\"], label=\"Val Loss\")\n    axes[0][i].set_title(f\"Seed {r['seed']} - Loss\")\n    axes[0][i].set_xlabel(\"Epoch\")\n    axes[0][i].set_ylabel(\"Loss\")\n    axes[0][i].legend()\n    axes[0][i].grid(True, alpha=0.3)\n\n    axes[1][i].plot(run_df[\"epoch\"], run_df[\"train_acc\"], label=\"Train Acc\")\n    axes[1][i].plot(run_df[\"epoch\"], run_df[\"val_acc\"], label=\"Val Acc\")\n    axes[1][i].set_title(f\"Seed {r['seed']} - Accuracy (Test: {r['test_acc']:.4f})\")\n    axes[1][i].set_xlabel(\"Epoch\")\n    axes[1][i].set_ylabel(\"Accuracy\")\n    axes[1][i].legend()\n    axes[1][i].grid(True, alpha=0.3)\n\nplt.suptitle(\"Stage 2e: Baseline Training (Labeled Data Only, He Init)\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a15f540c767eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}