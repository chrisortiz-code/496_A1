{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T00:39:21.775678Z",
     "start_time": "2026-02-16T00:39:20.230597Z"
    }
   },
   "source": [
    "%%capture\n!pip install numpy pandas matplotlib tensorflow keras torch scipy"
   ],
   "id": "setup",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stage 1: Data Preparation and Preprocessing\n\n**Dataset:** Fashion-MNIST (70,000 samples, 10 classes, 28x28 grayscale images)\n\n**Data Splitting:**\n- Test set: 10% — Validation set: 10% — Training pool: 80%\n\n**Semi-Supervised Setup** (from training pool):\n- 10% labeled (labels available) — 90% unlabeled (labels hidden)\n\n**Preprocessing Pipeline** (when `preprocess=True`):\n1. Split raw data first (no leakage)\n2. Compute normalization stats from **labeled train only**\n3. Apply Z-score normalization to all splits\n4. Augment **train only**: 50% horizontal flip + 50% 3x3 color jitter",
   "id": "stage1_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T00:40:15.221539Z",
     "start_time": "2026-02-16T00:39:21.775678Z"
    }
   },
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom DataLoader import DataLoader\n\nloader = DataLoader()\n\n# ---------------------------------------------------------\n# 1a. Preprocessing Comparison: Z-Score vs Min-Max\n# ---------------------------------------------------------\ndf_zscore = loader.get_z_score_df()\ndf_minmax = loader.get_min_max_df()\ndf_raw = loader.get_standard_df()\n\nprint(\"=\" * 60)\nprint(\"PREPROCESSING COMPARISON\")\nprint(\"=\" * 60)\n\nfor name, df in [(\"Raw (0-255)\", df_raw), (\"Z-Score\", df_zscore), (\"Min-Max\", df_minmax)]:\n    features = df[loader.feature_cols]\n    print(f\"\\n{name}:\")\n    print(f\"  Mean:  {features.mean().mean():.4f}\")\n    print(f\"  Std:   {features.std().mean():.4f}\")\n    print(f\"  Min:   {features.min().min():.4f}\")\n    print(f\"  Max:   {features.max().max():.4f}\")\n\n# ---------------------------------------------------------\n# 1b. Data Splitting with proper pipeline (preprocess=True)\n# ---------------------------------------------------------\ndata_dict = loader.prepare_data(\n    test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=1,\n    preprocess=True, normalize=\"z_score\"\n)\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"SEMI-SUPERVISED SPLIT (with preprocessing + augmentation)\")\nprint(f\"{'=' * 60}\")\nprint(f\"  Labeled train:   {len(data_dict['labeled_train']):,} (includes augmented)\")\nprint(f\"  Unlabeled train: {len(data_dict['unlabeled_train']):,}\")\nprint(f\"  Validation:      {len(data_dict['validation']):,}\")\nprint(f\"  Test:            {len(data_dict['test']):,}\")\n\n# ---------------------------------------------------------\n# 1c. Class Distribution\n# ---------------------------------------------------------\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Note: labeled_train now has 2x samples due to augmentation\noriginal_count = len(data_dict['labeled_train']) // 2\nprint(f\"\\nClass distribution (original labeled train, before aug):\")\n\n# ---------------------------------------------------------\n# 1d. Augmentation Demo (50% flip + 50% jitter)\n# ---------------------------------------------------------\nraw_data = loader.prepare_data(\n    test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=1,\n    preprocess=False  # Get raw for demo\n)\nsample = raw_data['labeled_train'].head(5)\naugmented = loader.augment_combined(sample, flip_prob=0.5, jitter_prob=0.5)\n\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i in range(5):\n    orig_img = sample.iloc[i][loader.feature_cols].values.reshape(28, 28)\n    aug_img = augmented.iloc[i][loader.feature_cols].values.reshape(28, 28)\n    axes[0][i].imshow(orig_img, cmap='gray'); axes[0][i].axis('off')\n    axes[1][i].imshow(aug_img, cmap='gray'); axes[1][i].axis('off')\n    if i == 0:\n        axes[0][i].set_ylabel('Original')\n        axes[1][i].set_ylabel('Augmented')\n\nplt.suptitle('Stage 1: Data Augmentation (50% Flip + 50% Jitter)', fontsize=13)\nplt.tight_layout()\nplt.show()",
   "id": "stage1_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. Shape: (70000, 785)\n",
      "============================================================\n",
      "PREPROCESSING COMPARISON\n",
      "============================================================\n",
      "\n",
      "Raw (0-255):\n",
      "  Mean:  72.9698\n",
      "  Std:   70.0766\n",
      "  Min:   0.0000\n",
      "  Max:   255.0000\n",
      "\n",
      "Z-Score:\n",
      "  Mean:  -0.0000\n",
      "  Std:   1.0000\n",
      "  Min:   -2.4116\n",
      "  Max:   194.0902\n",
      "\n",
      "Min-Max:\n",
      "  Mean:  0.2862\n",
      "  Std:   0.2750\n",
      "  Min:   0.0000\n",
      "  Max:   1.0000\n",
      "\n",
      "============================================================\n",
      "SEMI-SUPERVISED SPLIT (with preprocessing + augmentation)\n",
      "============================================================\n",
      "  Labeled train:   22,400 (includes augmented)\n",
      "  Unlabeled train: 44,800\n",
      "  Validation:      7,000\n",
      "  Test:            7,000\n",
      "\n",
      "Class distribution (original labeled train, before aug):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Supervised Neural Network Baseline\n",
    "\n",
    "**Architecture:** 784 → 1028 (ReLU) → 1028 (ReLU) → 10 (Softmax via CrossEntropyLoss)\n",
    "\n",
    "**Training:** SGD with mini-batch (batch=64), learning rate=0.01, momentum=0.04\n",
    "\n",
    "**Ablation Configurations** (each with 5 seeds for statistical rigor):\n",
    "\n",
    "| Config | Preprocess | Init | Model |\n",
    "|--------|-----------|------|-------|\n",
    "| `baseline` | No (raw) | normal | VanillaModel |\n",
    "| `+preprocessing` | Yes (Z-score + aug) | normal | VanillaModel |\n",
    "| `+he_init` | Yes | He | VanillaModel |\n",
    "\n",
    "**Note:** Preprocessing  includes normalization (train stats only) + augmentation (50% flip + 50% jitter on train only).\n",
    "\n",
    "## Overfitting Detection\n",
    "\n",
    "**Criterion:** $E_V^{(t)} > \\bar{E}_V + k \\cdot \\sigma_{E_V} \\cdot \\sqrt{t_0 / t}$"
   ],
   "id": "stage2_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T00:58:28.121375Z",
     "start_time": "2026-02-16T00:42:20.540567Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from DataLoader import DataLoader\n",
    "from VanillaModel import VanillaModel\n",
    "from Overfitting import OverfitDetector\n",
    "from Results import Results\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Seed-setting utility\n",
    "# ---------------------------------------------------------\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Results pipeline\n",
    "# ---------------------------------------------------------\n",
    "results = Results(base_dir=\"results\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Ablation configurations (simplified: preprocess = norm + aug)\n",
    "# ---------------------------------------------------------\n",
    "ABLATION_CONFIGS = [\n",
    "    {\"config\": \"baseline\",       \"preprocess\": False, \"init\": \"normal\"},\n",
    "    {\"config\": \"+preprocessing\", \"preprocess\": True,  \"init\": \"normal\"},\n",
    "    {\"config\": \"+he_init\",       \"preprocess\": True,  \"init\": \"he\"},\n",
    "]\n",
    "\n",
    "EPOCHS = 200\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.5\n",
    "SEEDS = [1, 42, 123, 456, 12345]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Generic training function\n",
    "# ---------------------------------------------------------\n",
    "def run_ablation(cfg, seed):\n",
    "    set_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    loader = DataLoader()\n",
    "\n",
    "    # prepare_data handles: split -> normalize (train stats) -> augment (train only)\n",
    "    data_dict = loader.prepare_data(\n",
    "        test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=seed,\n",
    "        preprocess=cfg[\"preprocess\"], normalize=\"z_score\"\n",
    "    )\n",
    "\n",
    "    loaders = loader.to_torch_loaders({\n",
    "        \"train\": data_dict[\"labeled_train\"],\n",
    "        \"validation\": data_dict[\"validation\"],\n",
    "        \"test\": data_dict[\"test\"]\n",
    "    }, batch_size=64)\n",
    "\n",
    "    # Determine tags for logging\n",
    "    norm_tag = \"z_score\" if cfg[\"preprocess\"] else \"raw\"\n",
    "    aug_tag = \"flip+jitter\" if cfg[\"preprocess\"] else \"none\"\n",
    "\n",
    "    results.begin_run(\n",
    "        seed=seed, weight_init=cfg[\"init\"], normalization=norm_tag,\n",
    "        augmentations=aug_tag, config=cfg[\"config\"], lr=LR, momentum=MOMENTUM\n",
    "    )\n",
    "\n",
    "    set_seed(seed)\n",
    "    model = VanillaModel(init_strategy=cfg[\"init\"], lr=LR, momentum=MOMENTUM).to(device)\n",
    "\n",
    "    overfit_detector = OverfitDetector(max_epochs=EPOCHS)\n",
    "    converged_epoch = EPOCHS\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = model.train_epoch(loaders[\"train\"], device)\n",
    "        val_loss, val_acc = model.evaluate(loaders[\"validation\"], device)\n",
    "        results.log_epoch(epoch + 1, train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "        val_error = 1.0 - val_acc\n",
    "        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error, epoch + 1)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            status = \" ** OVERFIT **\" if is_overfitting else \"\"\n",
    "            print(f\"      Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}{status}\")\n",
    "\n",
    "        if is_overfitting:\n",
    "            converged_epoch = epoch + 1\n",
    "            print(f\"      >>> Overfitting at epoch {converged_epoch}\")\n",
    "            break\n",
    "\n",
    "    _, test_acc = model.evaluate(loaders[\"test\"], device)\n",
    "    results.end_run(test_acc=test_acc, converged_epoch=converged_epoch)\n",
    "    return test_acc, converged_epoch\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Run all configs x seeds\n",
    "# ---------------------------------------------------------\n",
    "for cfg in ABLATION_CONFIGS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Config: {cfg['config']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for seed in SEEDS:\n",
    "        print(f\"   Seed {seed}...\")\n",
    "        acc, ep = run_ablation(cfg, seed)\n",
    "        print(f\"   -> Test Acc: {acc:.4f} | Epochs: {ep}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Summary\n",
    "# ---------------------------------------------------------\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"STAGE 2 ABLATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "results.summary()"
   ],
   "id": "stage2_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Config: baseline\n",
      "============================================================\n",
      "   Seed 1...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      Epoch 040 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      Epoch 060 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      Epoch 080 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      Epoch 100 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      Epoch 120 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      Epoch 140 | Train Acc: 0.1003 | Val Acc: 0.0984\n",
      "      >>> Overfitting at epoch 159\n",
      "   -> Test Acc: 0.0991 | Epochs: 159\n",
      "   Seed 42...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 040 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 060 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 080 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 100 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 120 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 140 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 160 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 180 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "      Epoch 200 | Train Acc: 0.0999 | Val Acc: 0.1040\n",
      "   -> Test Acc: 0.0951 | Epochs: 200\n",
      "   Seed 123...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 040 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 060 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 080 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 100 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 120 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 140 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 160 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 180 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "      Epoch 200 | Train Acc: 0.0993 | Val Acc: 0.1037\n",
      "   -> Test Acc: 0.1007 | Epochs: 200\n",
      "   Seed 456...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      Epoch 040 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      Epoch 060 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      Epoch 080 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      Epoch 100 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      Epoch 120 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      Epoch 140 | Train Acc: 0.0961 | Val Acc: 0.0997\n",
      "      >>> Overfitting at epoch 159\n",
      "   -> Test Acc: 0.1027 | Epochs: 159\n",
      "   Seed 12345...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 040 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 060 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 080 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 100 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 120 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 140 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 160 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 180 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "      Epoch 200 | Train Acc: 0.1007 | Val Acc: 0.0966\n",
      "   -> Test Acc: 0.0956 | Epochs: 200\n",
      "\n",
      "============================================================\n",
      "Config: +preprocessing\n",
      "============================================================\n",
      "   Seed 1...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9031 | Val Acc: 0.8601\n",
      "      >>> Overfitting at epoch 31\n",
      "   -> Test Acc: 0.8660 | Epochs: 31\n",
      "   Seed 42...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9030 | Val Acc: 0.8630\n",
      "      >>> Overfitting at epoch 30\n",
      "   -> Test Acc: 0.8660 | Epochs: 30\n",
      "   Seed 123...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9028 | Val Acc: 0.8639\n",
      "      >>> Overfitting at epoch 32\n",
      "   -> Test Acc: 0.8686 | Epochs: 32\n",
      "   Seed 456...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9025 | Val Acc: 0.8584\n",
      "      >>> Overfitting at epoch 28\n",
      "   -> Test Acc: 0.8637 | Epochs: 28\n",
      "   Seed 12345...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9024 | Val Acc: 0.8573\n",
      "      >>> Overfitting at epoch 36\n",
      "   -> Test Acc: 0.8676 | Epochs: 36\n",
      "\n",
      "============================================================\n",
      "Config: +he_init\n",
      "============================================================\n",
      "   Seed 1...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9834 | Val Acc: 0.8647\n",
      "      >>> Overfitting at epoch 27\n",
      "   -> Test Acc: 0.8676 | Epochs: 27\n",
      "   Seed 42...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9831 | Val Acc: 0.8624\n",
      "      >>> Overfitting at epoch 24\n",
      "   -> Test Acc: 0.8649 | Epochs: 24\n",
      "   Seed 123...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9846 | Val Acc: 0.8721\n",
      "      >>> Overfitting at epoch 28\n",
      "   -> Test Acc: 0.8740 | Epochs: 28\n",
      "   Seed 456...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      >>> Overfitting at epoch 17\n",
      "   -> Test Acc: 0.8571 | Epochs: 17\n",
      "   Seed 12345...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9830 | Val Acc: 0.8586\n",
      "      >>> Overfitting at epoch 38\n",
      "   -> Test Acc: 0.8680 | Epochs: 38\n",
      "\n",
      "============================================================\n",
      "STAGE 2 ABLATION SUMMARY\n",
      "============================================================\n",
      "              +he_init | Test Acc: 0.8663 +/- 0.0055 | Epochs: 26.8 +/- 6.8 | n=5\n",
      "        +preprocessing | Test Acc: 0.8664 +/- 0.0017 | Epochs: 31.4 +/- 2.7 | n=5\n",
      "              baseline | Test Acc: 0.0987 +/- 0.0029 | Epochs: 183.6 +/- 20.1 | n=5\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stage 3: Architecture Selection and Regularization\n\nTwo techniques implemented:\n1. **L1 Regularization** — encourages sparsity in weights\n2. **Magnitude-Based Pruning** — train oversized network, prune smallest weights, fine-tune\n\nBoth use Stage2Model with the loss function:\n$$\\mathcal{L}(w) = \\mathcal{L}_{CE} + \\lambda_1 |w|$$\n\n**RigL** (Evci et al., 2020) is an advancement on pruning: sparse-to-sparse training with topology updates.",
   "id": "stage3_header"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stage 3a: L1 Regularization\n\nAdds L1 regularization to the baseline pipeline. Uses Stage2Model with hidden_size=1028 (same as baseline) and no pruning — isolating the effect of regularization.",
   "id": "stage3a_reg_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T01:50:58.519406Z",
     "start_time": "2026-02-16T01:45:36.006724Z"
    }
   },
   "source": "import torch\nimport numpy as np\nfrom DataLoader import DataLoader\nfrom Stage2Model import Stage2Model\nfrom Overfitting import OverfitDetector\nfrom Results import Results\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nresults = Results(base_dir=\"results\")\n\nEPOCHS = 200\nLR = 0.01\nMOMENTUM = 0.04\nSEEDS = [1, 42, 123, 456, 12345]\nCONFIG_TAG = \"+l1_reg\"\n\ndef run_l1_reg(seed):\n    set_seed(seed)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    loader = DataLoader()\n    data_dict = loader.prepare_data(\n        test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=seed,\n        preprocess=True, normalize=\"z_score\"\n    )\n\n    loaders = loader.to_torch_loaders({\n        \"train\": data_dict[\"labeled_train\"],\n        \"validation\": data_dict[\"validation\"],\n        \"test\": data_dict[\"test\"]\n    }, batch_size=64)\n\n    results.begin_run(\n        seed=seed, weight_init=\"he\", normalization=\"z_score\",\n        augmentations=\"flip+jitter\", config=CONFIG_TAG, lr=LR, momentum=MOMENTUM\n    )\n\n    set_seed(seed)\n    model = Stage2Model(\n        hidden_size=1028, init_strategy=\"he\", lr=LR, momentum=MOMENTUM,\n        lambda1=1e-4\n    ).to(device)\n\n    overfit_detector = OverfitDetector(max_epochs=EPOCHS)\n    converged_epoch = EPOCHS\n\n    for epoch in range(EPOCHS):\n        train_loss, train_acc = model.train_epoch(loaders[\"train\"], device)\n        val_loss, val_acc = model.evaluate(loaders[\"validation\"], device)\n        results.log_epoch(epoch + 1, train_loss, train_acc, val_loss, val_acc)\n\n        val_error = 1.0 - val_acc\n        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error, epoch + 1)\n\n        if (epoch + 1) % 20 == 0:\n            status = \" ** OVERFIT **\" if is_overfitting else \"\"\n            print(f\"      Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}{status}\")\n\n        if is_overfitting:\n            converged_epoch = epoch + 1\n            print(f\"      >>> Overfitting at epoch {converged_epoch}\")\n            break\n\n    _, test_acc = model.evaluate(loaders[\"test\"], device)\n    results.end_run(test_acc=test_acc, converged_epoch=converged_epoch)\n    return test_acc, converged_epoch\n\n# ---------------------------------------------------------\n# Run 5 seeds\n# ---------------------------------------------------------\nprint(f\"{'='*60}\")\nprint(f\"Config: {CONFIG_TAG} (Stage2Model hidden=1028, L1 reg)\")\nprint(f\"{'='*60}\")\nfor seed in SEEDS:\n    print(f\"   Seed {seed}...\")\n    acc, ep = run_l1_reg(seed)\n    print(f\"   -> Test Acc: {acc:.4f} | Epochs: {ep}\")\n\nresults.summary()",
   "id": "stage3a_reg_code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Config: +l1_reg (Stage2Model hidden=1028, L1 reg)\n",
      "============================================================\n",
      "   Seed 1...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9747 | Val Acc: 0.8659\n",
      "      >>> Overfitting at epoch 25\n",
      "   -> Test Acc: 0.8663 | Epochs: 25\n",
      "   Seed 42...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      >>> Overfitting at epoch 17\n",
      "   -> Test Acc: 0.8634 | Epochs: 17\n",
      "   Seed 123...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9758 | Val Acc: 0.8731\n",
      "      >>> Overfitting at epoch 24\n",
      "   -> Test Acc: 0.8754 | Epochs: 24\n",
      "   Seed 456...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      >>> Overfitting at epoch 17\n",
      "   -> Test Acc: 0.8580 | Epochs: 17\n",
      "   Seed 12345...\n",
      "Data Loaded. Shape: (70000, 785)\n",
      "      Epoch 020 | Train Acc: 0.9745 | Val Acc: 0.8590\n",
      "      >>> Overfitting at epoch 36\n",
      "   -> Test Acc: 0.8703 | Epochs: 36\n",
      "               +l1_reg | Test Acc: 0.8667 +/- 0.0059 | Epochs: 23.8 +/- 7.0 | n=5\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stage 3a continued: Magnitude-Based Pruning\n\nAdds pruning on top of L1 regularization. Uses Stage2Model with:\n- **2x wider** network (hidden_size=2056)\n- L1 regularization during pre-training\n- Global magnitude pruning at **{10%, 25%, 50%}** rates\n- Fine-tuning after pruning\n\nThe ablation table uses the **best pruning rate** (25%) as the representative config.",
   "id": "stage3a_prune_header"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T02:27:05.689678Z",
     "start_time": "2026-02-16T01:54:23.259579Z"
    }
   },
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom DataLoader import DataLoader\nfrom Stage2Model import Stage2Model\nfrom Overfitting import OverfitDetector\nfrom Results import Results\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nresults = Results(base_dir=\"results\")\n\nPRETRAIN_EPOCHS = 200\nFINETUNE_EPOCHS = 50\nLR = 0.01\nMOMENTUM = 0.04\nPRUNE_RATES = [0.10, 0.25, 0.50]\nSEEDS = [1, 42, 123, 456, 12345]\n\ndef run_pruning(seed, prune_rate):\n    set_seed(seed)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    loader = DataLoader()\n    data_dict = loader.prepare_data(\n        test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=seed,\n        preprocess=True, normalize=\"z_score\"\n    )\n\n    loaders = loader.to_torch_loaders({\n        \"train\": data_dict[\"labeled_train\"],\n        \"validation\": data_dict[\"validation\"],\n        \"test\": data_dict[\"test\"]\n    }, batch_size=64)\n\n    config_tag = f\"+pruning_{int(prune_rate*100)}pct\"\n\n    # --- Phase 1: Pre-train ---\n    results.begin_run(\n        seed=seed, weight_init=\"he\", normalization=\"z_score\",\n        augmentations=\"flip+jitter\", config=f\"{config_tag}_pretrain\",\n        lr=LR, momentum=MOMENTUM\n    )\n\n    set_seed(seed)\n    model = Stage2Model(hidden_size=2056, init_strategy=\"he\",\n                        lr=LR, momentum=MOMENTUM, lambda1=1e-4).to(device)\n\n    overfit_detector = OverfitDetector(max_epochs=PRETRAIN_EPOCHS)\n    pretrain_converged = PRETRAIN_EPOCHS\n\n    for epoch in range(PRETRAIN_EPOCHS):\n        train_loss, train_acc = model.train_epoch(loaders[\"train\"], device)\n        val_loss, val_acc = model.evaluate(loaders[\"validation\"], device)\n        results.log_epoch(epoch + 1, train_loss, train_acc, val_loss, val_acc)\n\n        val_error = 1.0 - val_acc\n        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error, epoch + 1)\n\n        if (epoch + 1) % 20 == 0:\n            status = \" ** OVERFIT **\" if is_overfitting else \"\"\n            print(f\"      Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}{status}\")\n\n        if is_overfitting:\n            pretrain_converged = epoch + 1\n            print(f\"      >>> Stopped at epoch {pretrain_converged} (overfitting)\")\n            break\n\n    if pretrain_converged == PRETRAIN_EPOCHS:\n        print(f\"      >>> Completed {PRETRAIN_EPOCHS} epochs (no early stop)\")\n\n    _, pre_prune_test = model.evaluate(loaders[\"test\"], device)\n    results.end_run(test_acc=pre_prune_test, converged_epoch=pretrain_converged)\n\n    # --- Phase 2: Prune ---\n    model.prune(prune_rate)\n    _, post_prune_test = model.evaluate(loaders[\"test\"], device)\n    print(f\"      Post-prune ({prune_rate:.0%}): {post_prune_test:.4f} | Sparsity: {model.sparsity():.1%}\")\n\n    # --- Phase 3: Fine-tune ---\n    results.begin_run(\n        seed=seed, weight_init=\"he\", normalization=\"z_score\",\n        augmentations=\"flip+jitter\", config=config_tag, lr=LR, momentum=MOMENTUM\n    )\n\n    overfit_detector = OverfitDetector(max_epochs=FINETUNE_EPOCHS)\n    finetune_converged = FINETUNE_EPOCHS\n\n    for epoch in range(FINETUNE_EPOCHS):\n        train_loss, train_acc = model.train_epoch(loaders[\"train\"], device)\n        val_loss, val_acc = model.evaluate(loaders[\"validation\"], device)\n        results.log_epoch(epoch + 1, train_loss, train_acc, val_loss, val_acc,\n                          sparsity=model.sparsity())\n\n        val_error = 1.0 - val_acc\n        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error, epoch + 1)\n\n        if is_overfitting:\n            finetune_converged = epoch + 1\n            print(f\"      >>> Fine-tune stopped at epoch {finetune_converged}\")\n            break\n\n    if finetune_converged == FINETUNE_EPOCHS:\n        print(f\"      >>> Fine-tune completed {FINETUNE_EPOCHS} epochs\")\n\n    _, final_test = model.evaluate(loaders[\"test\"], device)\n    results.end_run(test_acc=final_test, converged_epoch=finetune_converged)\n\n    return {\n        \"seed\": seed, \"prune_rate\": prune_rate,\n        \"pre_prune_acc\": pre_prune_test,\n        \"post_prune_acc\": post_prune_test,\n        \"final_acc\": final_test,\n    }\n\n# ---------------------------------------------------------\n# Run all combinations\n# ---------------------------------------------------------\nall_pruning_results = []\n\nfor prune_rate in PRUNE_RATES:\n    print(f\"\\n{'='*60}\")\n    print(f\"Pruning {prune_rate:.0%}\")\n    print(f\"{'='*60}\")\n    for seed in SEEDS:\n        print(f\"   Seed {seed}...\")\n        r = run_pruning(seed, prune_rate)\n        all_pruning_results.append(r)\n        print(f\"   -> Pre: {r['pre_prune_acc']:.4f} | Post: {r['post_prune_acc']:.4f} | Fine-tuned: {r['final_acc']:.4f}\")\n\n# ---------------------------------------------------------\n# Summary\n# ---------------------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(f\"PRUNING RESULTS SUMMARY\")\nprint(f\"{'='*70}\")\nfor rate in PRUNE_RATES:\n    group = [r for r in all_pruning_results if r[\"prune_rate\"] == rate]\n    fine = np.array([r[\"final_acc\"] for r in group])\n    print(f\"  {rate:>5.0%} | Fine-tuned: {fine.mean():.4f} +/- {fine.std():.4f}\")\n\n# ---------------------------------------------------------\n# Plot\n# ---------------------------------------------------------\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.arange(len(PRUNE_RATES))\nwidth = 0.25\n\nfor i, (label, key) in enumerate([\n    (\"Pre-Prune\", \"pre_prune_acc\"),\n    (\"Post-Prune\", \"post_prune_acc\"),\n    (\"Fine-Tuned\", \"final_acc\")\n]):\n    means = [np.mean([r[key] for r in all_pruning_results if r[\"prune_rate\"] == rate]) for rate in PRUNE_RATES]\n    stds = [np.std([r[key] for r in all_pruning_results if r[\"prune_rate\"] == rate]) for rate in PRUNE_RATES]\n    ax.bar(x + i * width, means, width, yerr=stds, label=label, capsize=4)\n\nax.set_xlabel(\"Pruning Rate\")\nax.set_ylabel(\"Test Accuracy\")\nax.set_title(\"Stage 3a: Magnitude-Based Pruning (2x Hidden, L1 Regularization)\")\nax.set_xticks(x + width)\nax.set_xticklabels([f\"{int(r*100)}%\" for r in PRUNE_RATES])\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()",
   "id": "stage3a_prune_code",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stage 3b: RigL Sparse-to-Sparse Training\n\n**Approach (Evci et al., 2020):**\n1. Start with **2x wider** network (hidden_size=2056) + L1 regularization\n2. **Initial prune** to target sparsity using magnitude\n3. **Topology updates** every 100 steps: drop smallest active weights, grow where gradients are largest\n4. Total sparsity stays constant — only *which* weights are active changes\n5. Compare sparsity targets: **{10%, 25%, 50%}**",
   "id": "stage3b_header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom DataLoader import DataLoader\nfrom RigLModel import RigLModel\nfrom Overfitting import OverfitDetector\nfrom Results import Results\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nresults = Results(base_dir=\"results\")\n\nEPOCHS = 200\nLR = 0.01\nMOMENTUM = 0.04\nSPARSITY_TARGETS = [0.10, 0.25, 0.50]\nSEEDS = [1, 42, 123, 456, 12345]\n\ndef run_rigl(seed, sparsity_target):\n    set_seed(seed)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    loader = DataLoader()\n    data_dict = loader.prepare_data(\n        test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=seed,\n        preprocess=True, normalize=\"z_score\"\n    )\n\n    loaders = loader.to_torch_loaders({\n        \"train\": data_dict[\"labeled_train\"],\n        \"validation\": data_dict[\"validation\"],\n        \"test\": data_dict[\"test\"]\n    }, batch_size=64)\n\n    config_tag = f\"rigl_{int(sparsity_target*100)}pct\"\n    results.begin_run(\n        seed=seed, weight_init=\"he\", normalization=\"z_score\",\n        augmentations=\"flip+jitter\", config=config_tag, lr=LR, momentum=MOMENTUM\n    )\n\n    set_seed(seed)\n    model = RigLModel(\n        hidden_size=2056, init_strategy=\"he\", lr=LR, momentum=MOMENTUM,\n        sparsity_target=sparsity_target, update_interval=100, drop_fraction=0.20,\n        lambda1=1e-4\n    ).to(device)\n\n    model.init_sparse()\n\n    overfit_detector = OverfitDetector(max_epochs=EPOCHS)\n    converged_epoch = EPOCHS\n\n    for epoch in range(EPOCHS):\n        train_loss, train_acc = model.train_epoch(loaders[\"train\"], device)\n        val_loss, val_acc = model.evaluate(loaders[\"validation\"], device)\n        results.log_epoch(epoch + 1, train_loss, train_acc, val_loss, val_acc,\n                          sparsity=model.sparsity())\n\n        val_error = 1.0 - val_acc\n        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error, epoch + 1)\n\n        if (epoch + 1) % 20 == 0:\n            status = \" ** OVERFIT **\" if is_overfitting else \"\"\n            print(f\"      Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | \"\n                  f\"Val Acc: {val_acc:.4f} | Topo: {model._topology_updates}{status}\")\n\n        if is_overfitting:\n            converged_epoch = epoch + 1\n            break\n\n    _, test_acc = model.evaluate(loaders[\"test\"], device)\n    results.end_run(test_acc=test_acc, converged_epoch=converged_epoch)\n    return test_acc, converged_epoch, model._topology_updates\n\n# ---------------------------------------------------------\n# Run all\n# ---------------------------------------------------------\nall_rigl_results = []\nfor sparsity in SPARSITY_TARGETS:\n    print(f\"\\n{'='*60}\")\n    print(f\"RigL sparsity={sparsity:.0%}\")\n    print(f\"{'='*60}\")\n    for seed in SEEDS:\n        print(f\"   Seed {seed}...\")\n        acc, ep, topo = run_rigl(seed, sparsity)\n        all_rigl_results.append({\"sparsity\": sparsity, \"seed\": seed, \"test_acc\": acc})\n        print(f\"   -> Test Acc: {acc:.4f} | Epochs: {ep} | Topo updates: {topo}\")\n\n# Summary\nprint(f\"\\n{'='*60}\")\nfor sp in SPARSITY_TARGETS:\n    accs = np.array([r[\"test_acc\"] for r in all_rigl_results if r[\"sparsity\"] == sp])\n    print(f\"  RigL {sp:.0%}: {accs.mean():.4f} +/- {accs.std():.4f}\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 5))\nx = np.arange(len(SPARSITY_TARGETS))\nmeans = [np.mean([r[\"test_acc\"] for r in all_rigl_results if r[\"sparsity\"] == sp]) for sp in SPARSITY_TARGETS]\nstds = [np.std([r[\"test_acc\"] for r in all_rigl_results if r[\"sparsity\"] == sp]) for sp in SPARSITY_TARGETS]\nax.bar(x, means, 0.5, yerr=stds, capsize=5, color='steelblue')\nax.set_xlabel(\"Sparsity Target\")\nax.set_ylabel(\"Test Accuracy\")\nax.set_title(\"Stage 3b: RigL Sparse-to-Sparse (L1 Regularization)\")\nax.set_xticks(x)\nax.set_xticklabels([f\"{int(s*100)}%\" for s in SPARSITY_TARGETS])\nax.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null,
   "id": "stage3b_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stage 4: Semi-Supervised Learning (Label Propagation)\n\n**Method:** Zhu & Ghahramani (2002) Label Propagation + Neural Network\n\n**Approach:**\n1. Run label propagation on labeled + unlabeled data (kNN affinity graph with RBF kernel)\n2. Filter pseudo-labels by confidence threshold (keep high-confidence predictions)\n3. Train Stage2Model (L1 regularization) on labeled + pseudo-labeled data\n4. Compare against Stage 2 baseline to measure benefit of pseudo-labels",
   "id": "stage4_header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom DataLoader import DataLoader\nfrom SemiSupervisedModel import LabelPropagation\nfrom Stage2Model import Stage2Model\nfrom Overfitting import OverfitDetector\nfrom Results import Results\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nresults = Results(base_dir=\"results\")\n\nEPOCHS = 200\nLR = 0.01\nMOMENTUM = 0.04\nSEEDS = [1, 42, 123, 456, 12345]\nCONFIDENCE_THRESHOLD = 0.80\nCONFIG_TAG = \"+ssl\"\n\ndef run_semi_supervised(seed):\n    set_seed(seed)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    loader = DataLoader()\n\n    # For SSL, we need access to unlabeled data BEFORE augmentation\n    raw_splits = loader.get_semi_supervised_split(\n        loader.df, test_size=0.10, val_size=0.10, labeled_ratio=0.20, seed=seed\n    )\n\n    # Compute normalization stats from labeled train\n    train_features = raw_splits[\"labeled_train\"][loader.feature_cols]\n    mu = train_features.mean()\n    sigma = train_features.std()\n    sigma[sigma == 0] = 1\n\n    # Apply normalization to all splits\n    def apply_norm(df):\n        df_out = df.copy()\n        df_out[loader.feature_cols] = (df[loader.feature_cols] - mu) / sigma\n        return df_out\n\n    normalized = {k: apply_norm(v) for k, v in raw_splits.items()}\n\n    labeled_train = normalized[\"labeled_train\"]\n    unlabeled_train = normalized[\"unlabeled_train\"]\n    print(f\"   Labeled: {len(labeled_train)} | Unlabeled: {len(unlabeled_train)}\")\n\n    # --- Label Propagation ---\n    X_labeled, y_labeled = loader.to_numpy(labeled_train, one_hot=False)\n    X_unlabeled = unlabeled_train[loader.feature_cols].values.astype(np.float32)\n\n    lp = LabelPropagation(sigma=1.0, max_iter=50, k_neighbors=10, num_classes=10)\n    pseudo_labels_soft = lp.propagate(X_labeled, y_labeled, X_unlabeled)\n\n    # Filter by confidence\n    pseudo_confidence = pseudo_labels_soft.max(axis=1)\n    pseudo_hard = pseudo_labels_soft.argmax(axis=1)\n    confident_mask = pseudo_confidence >= CONFIDENCE_THRESHOLD\n    n_confident = confident_mask.sum()\n    print(f\"   Pseudo-labels above {CONFIDENCE_THRESHOLD}: \"\n          f\"{n_confident}/{len(pseudo_confidence)} ({100*n_confident/len(pseudo_confidence):.1f}%)\")\n\n    # Build combined training set\n    X_pseudo = X_unlabeled[confident_mask]\n    y_pseudo = pseudo_hard[confident_mask]\n    pseudo_df = pd.DataFrame(X_pseudo, columns=loader.feature_cols)\n    pseudo_df[\"label\"] = y_pseudo\n\n    combined_train = pd.concat([labeled_train, pseudo_df], axis=0).reset_index(drop=True)\n\n    # Apply augmentation (50% flip + 50% jitter)\n    augmented = loader.augment_combined(combined_train, flip_prob=0.5, jitter_prob=0.5)\n    final_train = pd.concat([combined_train, augmented], axis=0).reset_index(drop=True)\n\n    print(f\"   Final training set: {len(final_train)}\")\n\n    loaders = loader.to_torch_loaders({\n        \"train\": final_train,\n        \"validation\": normalized[\"validation\"],\n        \"test\": normalized[\"test\"]\n    }, batch_size=64)\n\n    results.begin_run(\n        seed=seed, weight_init=\"he\", normalization=\"z_score\",\n        augmentations=\"flip+jitter+ssl_lp\", config=CONFIG_TAG, lr=LR, momentum=MOMENTUM\n    )\n\n    set_seed(seed)\n    model = Stage2Model(\n        hidden_size=1028, init_strategy=\"he\", lr=LR, momentum=MOMENTUM,\n        lambda1=1e-4\n    ).to(device)\n\n    overfit_detector = OverfitDetector(max_epochs=EPOCHS)\n    converged_epoch = EPOCHS\n\n    for epoch in range(EPOCHS):\n        train_loss, train_acc = model.train_epoch(loaders[\"train\"], device)\n        val_loss, val_acc = model.evaluate(loaders[\"validation\"], device)\n        results.log_epoch(epoch + 1, train_loss, train_acc, val_loss, val_acc)\n\n        val_error = 1.0 - val_acc\n        is_overfitting, mean_ev, std_ev = overfit_detector.check(val_error, epoch + 1)\n\n        if (epoch + 1) % 20 == 0:\n            status = \" ** OVERFIT **\" if is_overfitting else \"\"\n            print(f\"      Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}{status}\")\n\n        if is_overfitting:\n            converged_epoch = epoch + 1\n            break\n\n    _, test_acc = model.evaluate(loaders[\"test\"], device)\n    results.end_run(test_acc=test_acc, converged_epoch=converged_epoch)\n    return test_acc, converged_epoch, n_confident\n\n# ---------------------------------------------------------\n# Run 5 seeds\n# ---------------------------------------------------------\nprint(f\"{'='*60}\")\nprint(f\"Config: {CONFIG_TAG} (Label Propagation + Stage2Model)\")\nprint(f\"{'='*60}\")\nfor seed in SEEDS:\n    print(f\"\\n   Seed {seed}...\")\n    acc, ep, n_pseudo = run_semi_supervised(seed)\n    print(f\"   -> Test Acc: {acc:.4f} | Epochs: {ep} | Pseudo used: {n_pseudo}\")\n\nresults.summary()",
   "outputs": [],
   "execution_count": null,
   "id": "stage4_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Comprehensive Analysis and Ablation Study\n\n## Component Contribution Analysis\n\nProgressive ablation table showing how each technique contributes to overall performance.\n\n## Statistical Analysis\n- **5 seeds per config** → df=4, $t_{0.05, 4} = 2.132$ for 90% confidence intervals\n- **Paired t-tests** between consecutive configurations to test significance\n- **90% CI formula:** $\\mu \\pm t \\cdot \\frac{\\sigma}{\\sqrt{n}}$"
   ],
   "id": "stage5_header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom Results import Results\nfrom scipy import stats\n\nresults = Results(base_dir=\"results\")\n\n# ---------------------------------------------------------\n# Ablation config order\n# ---------------------------------------------------------\nABLATION_ORDER = [\n    \"baseline\",\n    \"+preprocessing\",\n    \"+he_init\",\n    \"+l1_reg\",\n    \"+pruning_25pct\",\n    \"+ssl\",\n]\n\nABLATION_LABELS = [\n    \"Baseline (raw, normal init)\",\n    \"+ Preprocessing (Z-score + aug)\",\n    \"+ Better Init (He)\",\n    \"+ L1 Regularization\",\n    \"+ Pruning (25%)\",\n    \"+ Semi-Supervised (LP)\",\n]\n\n# ---------------------------------------------------------\n# Build ablation table\n# ---------------------------------------------------------\ntable = results.ablation_table(ABLATION_ORDER)\ntable[\"label\"] = ABLATION_LABELS\n\nprint(\"=\" * 90)\nprint(\"COMPONENT CONTRIBUTION ANALYSIS\")\nprint(\"=\" * 90)\nprint(f\"{'Configuration':<35} | {'Test Accuracy':>16} | {'Improvement':>12} | {'p-value':>10}\")\nprint(f\"{'-'*35}-+-{'-'*16}-+-{'-'*12}-+-{'-'*10}\")\n\nfor _, row in table.iterrows():\n    acc_str = f\"{row['mean_acc']:.4f} +/- {row['std_acc']:.4f}\" if not np.isnan(row['mean_acc']) else \"N/A\"\n    imp_str = f\"+{row['improvement_pct']:.2f}%\" if row['improvement_pct'] > 0 else (\n        f\"{row['improvement_pct']:.2f}%\" if not np.isnan(row['improvement_pct']) else \"---\"\n    )\n    p_str = f\"{row['p_value']:.4f}\" if not np.isnan(row['p_value']) else \"---\"\n    sig = \" *\" if not np.isnan(row['p_value']) and row['p_value'] < 0.10 else \"\"\n    print(f\"{row['label']:<35} | {acc_str:>16} | {imp_str:>12} | {p_str:>8}{sig}\")\n\nprint(f\"\\n* = statistically significant at p < 0.10\")\n\n# ---------------------------------------------------------\n# 90% Confidence Intervals\n# ---------------------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(f\"90% CONFIDENCE INTERVALS (t-distribution, df=4)\")\nprint(f\"{'='*70}\")\n\nci_data = []\nfor cfg, label in zip(ABLATION_ORDER, ABLATION_LABELS):\n    mean, lower, upper, t_val, n = results.confidence_interval(cfg, confidence=0.90)\n    ci_data.append({\"config\": cfg, \"label\": label, \"mean\": mean, \"lower\": lower, \"upper\": upper, \"n\": n})\n    if not np.isnan(lower):\n        print(f\"  {label:<35} | {mean:.4f} [{lower:.4f}, {upper:.4f}] (n={n})\")\n    else:\n        print(f\"  {label:<35} | N/A\")\n\n# ---------------------------------------------------------\n# Plot: Ablation bar chart with 90% CI error bars\n# ---------------------------------------------------------\nci_df = pd.DataFrame(ci_data)\nvalid = ci_df.dropna(subset=[\"mean\"])\n\nfig, ax = plt.subplots(figsize=(12, 6))\nx = np.arange(len(valid))\nmeans = valid[\"mean\"].values\nlower_err = means - valid[\"lower\"].values\nupper_err = valid[\"upper\"].values - means\nerrors = np.array([lower_err, upper_err])\n\nbars = ax.bar(x, means, 0.6, yerr=errors, capsize=5, color='steelblue', edgecolor='navy', alpha=0.85)\nax.set_xlabel(\"Configuration\", fontsize=12)\nax.set_ylabel(\"Test Accuracy\", fontsize=12)\nax.set_title(\"Ablation Study: Component Contribution (90% CI)\", fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(valid[\"label\"].values, rotation=30, ha='right', fontsize=9)\nax.grid(True, alpha=0.3, axis='y')\n\nfor i, (bar, m) in enumerate(zip(bars, means)):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003,\n            f'{m:.4f}', ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# ---------------------------------------------------------\n# RigL comparison\n# ---------------------------------------------------------\nprint(f\"\\n{'='*70}\")\nprint(f\"RIGL COMPARISON\")\nprint(f\"{'='*70}\")\nfor sp in [10, 25, 50]:\n    cfg = f\"rigl_{sp}pct\"\n    accs = results.get_config_accs(cfg)\n    if len(accs) > 0:\n        print(f\"  RigL {sp}%: {accs.mean():.4f} +/- {accs.std():.4f} (n={len(accs)})\")",
   "outputs": [],
   "execution_count": null,
   "id": "stage5_code"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}